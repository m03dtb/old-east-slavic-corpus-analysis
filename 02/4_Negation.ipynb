{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caad469c-1388-4583-a022-010c620a0f02",
   "metadata": {},
   "source": [
    "# File 02/4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6759dbe9-7233-4b9c-9320-ed275f776ec9",
   "metadata": {},
   "source": [
    "# DESCRIPTION:\n",
    "Mark verbs as negated if they fall within the scope of a negation particle.\n",
    "Record the negation particle in the column Negation_Marker.\n",
    "\n",
    "# INPUT_FILE:\n",
    "\n",
    "./OUTPUTS/dataframe_02_3.csv\n",
    "\n",
    "# OUTPUT_FILE:\n",
    "./OUTPUTS/dataframe_02_4.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d047bdfe-9ef1-4409-9705-656394e762bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c20e4ed4-3a1b-4710-9b22-5e7d6795e84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv\n",
    "csv_path = 'OUTPUTS/dataframe_02_3.csv'\n",
    "df = pd.read_csv(csv_path, low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f45f45-7a8c-43da-bb5a-6bdde02246ef",
   "metadata": {},
   "source": [
    "# Drop columns beginning with \"Unnamend ...\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e626cf1-be2b-406c-aefa-0c8ce95c3749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unnamed_columns(df):\n",
    "    # Delete columns named \"Unnamed\"\n",
    "    return df.loc[:, ~df.columns.str.startswith(\"Unnamed\")]\n",
    "df = drop_unnamed_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aea7fcb-f57f-460f-bee3-2455662d918a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['File', 'Text Title', 'Language', 'Sentence ID', 'Token ID', 'Form',\n",
      "       'Lemma', 'Lemma_norm', 'POS', 'Morphology', 'Head ID', 'Relation',\n",
      "       'Presentation After', 'Russian Translation', 'English Translation',\n",
      "       'Type', 'century', 'exact', 'lang', 'source', 'place', 'region'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['mst', 'mstislav-col', 'birchbark', 'pskov', 'const', 'luk-koloc',\n",
       "       'lav', 'smol-pol-lit', 'nov-sin', 'avv', 'kiev-hyp', 'peter',\n",
       "       'vest-kur', 'spi', 'zadon', 'rusprav', 'pskov-ivan',\n",
       "       'rig-smol1281', 'drac', 'sergrad', 'nov-list', 'ostromir-col',\n",
       "       'varlaam', 'afnik', 'dux-grjaz', 'ust-vlad', 'riga-goth', 'domo',\n",
       "       'usp-sbor', 'schism', 'nov-marg', 'suz-lav', 'novgorod-jaroslav',\n",
       "       'pvl-hyp'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.columns)\n",
    "df.File.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "242bf199-c7db-4d81-b9a5-ad0944bce628",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cols_before = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f575f6ff-afbe-4312-97ab-6a9c00f368d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Text Title</th>\n",
       "      <th>Language</th>\n",
       "      <th>Sentence ID</th>\n",
       "      <th>Token ID</th>\n",
       "      <th>Form</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>Lemma_norm</th>\n",
       "      <th>POS</th>\n",
       "      <th>Morphology</th>\n",
       "      <th>...</th>\n",
       "      <th>Presentation After</th>\n",
       "      <th>Russian Translation</th>\n",
       "      <th>English Translation</th>\n",
       "      <th>Type</th>\n",
       "      <th>century</th>\n",
       "      <th>exact</th>\n",
       "      <th>lang</th>\n",
       "      <th>source</th>\n",
       "      <th>place</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mst</td>\n",
       "      <td>Mstislav’s letter</td>\n",
       "      <td>orv</td>\n",
       "      <td>189407</td>\n",
       "      <td>2157773</td>\n",
       "      <td>Се</td>\n",
       "      <td>се</td>\n",
       "      <td>се</td>\n",
       "      <td>I-</td>\n",
       "      <td>---------n</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>вот, это</td>\n",
       "      <td>behold, here is</td>\n",
       "      <td>OR</td>\n",
       "      <td>12</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>OR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Novgorod</td>\n",
       "      <td>East Slavic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mst</td>\n",
       "      <td>Mstislav’s letter</td>\n",
       "      <td>orv</td>\n",
       "      <td>189407</td>\n",
       "      <td>2157774</td>\n",
       "      <td>азъ</td>\n",
       "      <td>азъ</td>\n",
       "      <td>аз_</td>\n",
       "      <td>Pp</td>\n",
       "      <td>1s---mn--i</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>я</td>\n",
       "      <td>I</td>\n",
       "      <td>OR</td>\n",
       "      <td>12</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>OR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Novgorod</td>\n",
       "      <td>East Slavic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  File         Text Title Language  Sentence ID  Token ID Form Lemma  \\\n",
       "0  mst  Mstislav’s letter      orv       189407   2157773   Се    се   \n",
       "1  mst  Mstislav’s letter      orv       189407   2157774  азъ   азъ   \n",
       "\n",
       "  Lemma_norm POS  Morphology  ...  Presentation After Russian Translation  \\\n",
       "0         се  I-  ---------n  ...                                вот, это   \n",
       "1        аз_  Pp  1s---mn--i  ...                                       я   \n",
       "\n",
       "  English Translation Type century   exact  lang  source     place  \\\n",
       "0     behold, here is   OR      12  1130.0    OR     NaN  Novgorod   \n",
       "1                   I   OR      12  1130.0    OR     NaN  Novgorod   \n",
       "\n",
       "        region  \n",
       "0  East Slavic  \n",
       "1  East Slavic  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf23c36-2b4d-4675-99b8-675f513c0a72",
   "metadata": {},
   "source": [
    "# Define \"Negation\" particles; parse Head and Token IDs as Integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcd1282c-a013-45a8-996e-7a8a84624816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Parse IDs as Integers\n",
    "df['Token ID'] = pd.to_numeric(df['Token ID'], errors='coerce').astype('Int64')\n",
    "df['Head ID']  = pd.to_numeric(df['Head ID'],  errors='coerce').astype('Int64')\n",
    "\n",
    "# 2. Initiate column \"Negation\" with values of Type Bool \n",
    "# Set default to \"False\"\n",
    "df['Negation'] = False\n",
    "df['Negation_Marker'] = ''\n",
    "\n",
    "# 3. Define negation particles\n",
    "negations = {'не','ни'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "266a9666-e781-4731-9974-764eeb5d944e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Negation', 'Negation_Marker'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the new cols were added \n",
    "df_cols_new = df.columns\n",
    "df_cols_added = df_cols_new.difference(df_cols_before)\n",
    "df_cols_added"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39311da-e913-41b1-98fc-881ac063a2ef",
   "metadata": {},
   "source": [
    "## Rearrange the order of columns in the DataFrame, delete Column \"source\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59b55bf3-d6f1-4f22-ac39-90411cbb3048",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_at_end = [\"Russian Translation \", \"English Translation \", \"place\", \"trans\", \"source\"]\n",
    "df = df[[c for c in df if c not in cols_at_end] \n",
    "        + [c for c in cols_at_end if c in df and c != \"source\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4187accd-58c0-4386-ab10-39ec617f0d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['File', 'Text Title', 'Language', 'Sentence ID', 'Token ID', 'Form',\n",
       "       'Lemma', 'Lemma_norm', 'POS', 'Morphology', 'Head ID', 'Relation',\n",
       "       'Presentation After', 'Russian Translation', 'English Translation',\n",
       "       'Type', 'century', 'exact', 'lang', 'region', 'Negation',\n",
       "       'Negation_Marker', 'place'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c45b3c1-e7fd-4bfb-935c-647643d8e99d",
   "metadata": {},
   "source": [
    "# Create Morphology Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68decfd-67f1-41e9-a9ce-27b4bc8a3816",
   "metadata": {},
   "source": [
    "## FUNCTION CALLS in \n",
    "- build_phrase\n",
    "- build_children_map\n",
    "- process_sentence\n",
    "\n",
    "1. call process_sentence(group) # \"process_sentence\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6bffff5-73e4-455f-bfd1-f97889dc734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_children_map(group):\n",
    "    \"\"\"\n",
    "    Create a Tree containing the relations between Token ID, Head ID and Relation\n",
    "\n",
    "    Output: \n",
    "        Verb's \"Token ID\"                   {'attribute1': ['Token ID'], 'attribute2': ['Token ID'], ...  }\n",
    "    Output Example:                   \n",
    "        2157784 defaultdict(<class 'list'>, {'sub': [2157774], 'xadv': [2157778], 'aux': [2157785], 'obl': [2157786], 'xobj': [2157789]})\n",
    "    \"\"\"\n",
    "    # create dict[dict[list]]\n",
    "    children = defaultdict(lambda: defaultdict(list))\n",
    "    # for all VERBS (i.e. \"group\")\n",
    "    for _, row in group.iterrows():\n",
    "        # determine Token ID, Head ID, Relation\n",
    "        token_id = int(row['Token ID'])\n",
    "        head_id = row['Head ID']\n",
    "        rel = row['Relation']\n",
    "        # exclude sentences where no Head ID exists \n",
    "        if pd.isna(head_id) or head_id == 0:\n",
    "            continue\n",
    "        parent_id = int(head_id)\n",
    "        # for each verb's \"Head ID\"  -> \n",
    "        children[parent_id][rel].append(token_id)\n",
    "\n",
    "    return children\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d720b7eb-1336-40b9-a936-916c4bad6770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_phrase(token_id, children, group):\n",
    "    \"\"\"\n",
    "    Return a list of partial sentences (phrase_parts) containing \n",
    "    [Subject] [быти‐Aux + eventually Negation + Verb] [Objects + its Attributes]\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Zeile für dieses Token holen\n",
    "    row = group.loc[group['Token ID'] == token_id].iloc[0]\n",
    "    form = row['Form']\n",
    "    if pd.isna(form) and not pd.isna(row.get('empty-token-sort')):\n",
    "        form = f\"<{row['empty-token-sort']}-V>\"\n",
    "    elif pd.isna(form):\n",
    "        form = ''\n",
    "\n",
    "    # 2.1 Subjekt (sub, xsub)\n",
    "    subs = (children.get(token_id, {}).get('sub', []) +\n",
    "            children.get(token_id, {}).get('xsub', []))\n",
    "    subject_phrase = ''\n",
    "    if subs:\n",
    "        parts_sub = []\n",
    "        for sub_id in subs:\n",
    "            # rekursiver Aufruf für Unter-Satz/Subjekt\n",
    "            parts_sub.append(' '.join(build_phrase(sub_id, children, group)))\n",
    "        subject_phrase = ' '.join(parts_sub)\n",
    "\n",
    "    # 2.2a Hilfsverb 'быти' (aux)\n",
    "    bytis = []\n",
    "    for aux_id in children.get(token_id, {}).get('aux', []):\n",
    "        aux_row = group.loc[group['Token ID'] == aux_id].iloc[0]\n",
    "        if aux_row['Lemma'] == 'быти':\n",
    "            bytis.append(aux_row['Form'])  # z.B. 'ѥсмь'\n",
    "\n",
    "    # 2.2b ggf. Negation (aux + Lemma in {\"не\",\"ни\"})\n",
    "    negs = []\n",
    "    for aux_id in children.get(token_id, {}).get('aux', []):\n",
    "        aux_row = group.loc[group['Token ID'] == aux_id].iloc[0]\n",
    "        if aux_row['Lemma'] in {'не', 'ни'}:\n",
    "            negs.append(aux_row['Form'])\n",
    "\n",
    "    # 2.3 Objekt(e) (obj, xobj)\n",
    "    obj_ids = (children.get(token_id, {}).get('obj', []) +\n",
    "               children.get(token_id, {}).get('xobj', []))\n",
    "    object_phrases = []\n",
    "    for obj_id in obj_ids:\n",
    "        # rekursiver Aufruf für Objekt‐Phrase\n",
    "        object_phrases.append(' '.join(build_phrase(obj_id, children, group)))\n",
    "\n",
    "    # 2.4 Attribute (atr)\n",
    "    atr_ids = children.get(token_id, {}).get('atr', [])\n",
    "    attr_phrases = []\n",
    "    for atr_id in atr_ids:\n",
    "        # rekursiver Aufruf für Attribute\n",
    "        attr_phrases.append(' '.join(build_phrase(atr_id, children, group)))\n",
    "\n",
    "    # 2.5 Zusammengesetztes Verb (Aux + Negation + Verb‐Form)\n",
    "    parts_verb = []\n",
    "    if bytis:\n",
    "        parts_verb += bytis        # z.B. ['ѥсмь']\n",
    "    if negs:\n",
    "        parts_verb += negs         # z.B. ['не']\n",
    "    parts_verb.append(form)       # z.B. ['ѥсмь', 'не', 'повелѣлъ'] oder ['ѥсмь', 'повелѣлъ']\n",
    "    verb_phrase = ' '.join(parts_verb)\n",
    "\n",
    "    # 2.6 Falls Attribute (Atr) zum Verb selbst existieren, anhängen\n",
    "    if attr_phrases:\n",
    "        # Mehrere Attribute in Klammern, durch Komma getrennt\n",
    "        verb_phrase += ' (' + ', '.join(attr_phrases) + ')'\n",
    "\n",
    "    # 2.7 Endgültige Phrasen‐Liste zusammenbauen\n",
    "    phrase_parts = []\n",
    "    if subject_phrase:\n",
    "        phrase_parts.append(subject_phrase)\n",
    "    phrase_parts.append(verb_phrase)\n",
    "    if object_phrases:\n",
    "        phrase_parts.append(', '.join(object_phrases))\n",
    "\n",
    "    return phrase_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "455080fb-11f7-4896-b7ca-f19ea3a402fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence(group):\n",
    "    \"\"\"\n",
    "    - Check if a sentence contains negation elemenets, mark the sentence with \n",
    "    bool \"Negation\" and – if sentence is negated – write the Negation Marker(s) to\n",
    "    column \"Negation_Marker\"\n",
    "    - Get the sentence's tree structure \n",
    "\n",
    "    Calls function \"build_children_map\"\n",
    "    Calls function \"build_phrase\"\n",
    "    \"\"\"\n",
    "    # --- A) Check for negation and WRITE in the NEGATED VERB's COLUMN \"Negation\" == True\n",
    "    # and \"Negation_Marker\": the negation marker ---\n",
    "    negations = {'не','ни'}\n",
    "    group = group.copy()\n",
    "    # MASK: contains all verbs (df[POS]=\"V-\")\n",
    "    mask_verbs = group['POS'].fillna('').str.startswith('V-')\n",
    "    for idx, verb in group[mask_verbs].iterrows():\n",
    "        # get \"Token ID\"\n",
    "        token_id = verb['Token ID']\n",
    "        # Filter -> get negations corresponding to the verb:  \n",
    "        negs = group.loc[\n",
    "            # conditions for the negation marker: \n",
    "            # Lemma must be in the negations list \n",
    "            (group['Lemma'].isin(negations)) &\n",
    "            # negation marker's \"Head ID\" must be the corresponding verb's token_id\n",
    "            (group['Head ID'] == token_id) &\n",
    "            # the negation marker's \"Relation\" must be \"aux\" (i.e. \"auxiliary\")\n",
    "            (group['Relation'].isin(['aux'])),\n",
    "            :\n",
    "        ]\n",
    "        # if the \"negs\" filter conditions are True: \n",
    "        if not negs.empty:\n",
    "            # in the column of the negated verb: \n",
    "            # mark col \"Negation\" as True\n",
    "            group.at[idx, 'Negation'] = True\n",
    "            # write the Lemma(s) of \"negs\" to the negated verb's col \"Negation_Marker\" \n",
    "            group.at[idx, 'Negation_Marker'] = ', '.join(negs['Lemma'].dropna())\n",
    "\n",
    "    # --- B) Get the Tree structure by calling \"build_children_map\" for each VERB ---\n",
    "    children = build_children_map(group)\n",
    "\n",
    "    # DEBUG: PRINT\n",
    "    #print(\"CHILDREN\")\n",
    "    #for k,v in children.items():\n",
    "    #    print(k, v)\n",
    "    \n",
    "    # --- C) Determine the root verb ---\n",
    "    # Search for Token where \"Relation\" is \"pred\" (predicate) AND which \"Head ID\" is \n",
    "    # none or NaN \n",
    "    root = None\n",
    "    for _, row in group.iterrows():\n",
    "        if row['Relation'] == 'pred' and (pd.isna(row['Head ID']) or int(row['Head ID']) == 0):\n",
    "            root = int(row['Token ID'])\n",
    "            break\n",
    "    \n",
    "    # As sometimes a root cannot be found in the sentences of the DF: \n",
    "    # If no root is found, assume the predicate is head of the sentence, i.e. \n",
    "    # where Relation=\"pred\"\n",
    "    if root is None:\n",
    "        possible = group.loc[group['Relation']=='pred']\n",
    "        if not possible.empty:\n",
    "            root = int(possible.iloc[0]['Token ID'])\n",
    "    \n",
    "    # --- D) Create the tree ---\n",
    "    sentence_str = ''\n",
    "    if root is not None:\n",
    "        phrase_parts = build_phrase(root, children, group)\n",
    "        # phrase_parts is stored as a list of strings \n",
    "        sentence_str = ' '.join([p for p in phrase_parts if p.strip() != ''])\n",
    "    \n",
    "    # --- E) Store the Tree Diagram in a separate Column ---\n",
    "    group['Sentence_Text'] = sentence_str\n",
    "    \n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa05ab69-3328-48f5-b3b0-5aa0ab1af10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ps/86qydyw53xj72p_g1g25hfx00000gn/T/ipykernel_19835/3674297392.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(process_sentence)\n"
     ]
    }
   ],
   "source": [
    "# Apply fn \"process_sentence\" to all verbs\n",
    "df = (\n",
    "    df\n",
    "    .groupby('Sentence ID', group_keys=False)\n",
    "    .apply(process_sentence)\n",
    ")\n",
    "\n",
    "# 5. Write df to file\n",
    "df.to_csv('OUTPUTS/data_with_nested_phrases.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5569dff7-263f-452e-a910-5d5bef8cf3f4",
   "metadata": {},
   "source": [
    "# Example for a sentence with negation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e4220ca-6253-4d05-9af9-ffa082656773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Find the first Sentence ID for a sentence containing negation \n",
    "first_neg_sent = df.loc[df[\"Negation\"], \"Sentence ID\"].iloc[4]\n",
    "\n",
    "# 2. Filter all cols with this Sentence ID \n",
    "first_sentence_df = df[df[\"Sentence ID\"] == first_neg_sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0438bd6-9424-48f0-9e64-1e95a3a8a196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Form</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>Sentence ID</th>\n",
       "      <th>Token ID</th>\n",
       "      <th>POS</th>\n",
       "      <th>Head ID</th>\n",
       "      <th>Negation</th>\n",
       "      <th>Negation_Marker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>а</td>\n",
       "      <td>а</td>\n",
       "      <td>210175</td>\n",
       "      <td>2287614</td>\n",
       "      <td>C-</td>\n",
       "      <td>2287829</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>четъ</td>\n",
       "      <td>чьто</td>\n",
       "      <td>210175</td>\n",
       "      <td>2287615</td>\n",
       "      <td>G-</td>\n",
       "      <td>2287829</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>ѡмьшҍ</td>\n",
       "      <td>омешь</td>\n",
       "      <td>210175</td>\n",
       "      <td>2287616</td>\n",
       "      <td>Nb</td>\n",
       "      <td>2287617</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>пришлю</td>\n",
       "      <td>присълати</td>\n",
       "      <td>210175</td>\n",
       "      <td>2287617</td>\n",
       "      <td>V-</td>\n",
       "      <td>2287615</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>и</td>\n",
       "      <td>и</td>\n",
       "      <td>210175</td>\n",
       "      <td>2287618</td>\n",
       "      <td>C-</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>вꙑ</td>\n",
       "      <td>вы</td>\n",
       "      <td>210175</td>\n",
       "      <td>2287619</td>\n",
       "      <td>Pp</td>\n",
       "      <td>2287624</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>имъ</td>\n",
       "      <td>и</td>\n",
       "      <td>210175</td>\n",
       "      <td>2287620</td>\n",
       "      <td>Pp</td>\n",
       "      <td>2287624</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>къне</td>\n",
       "      <td>конь</td>\n",
       "      <td>210175</td>\n",
       "      <td>2287621</td>\n",
       "      <td>Nb</td>\n",
       "      <td>2287624</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>мъи</td>\n",
       "      <td>мои</td>\n",
       "      <td>210175</td>\n",
       "      <td>2287622</td>\n",
       "      <td>Pp</td>\n",
       "      <td>2287621</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>голубꙑи</td>\n",
       "      <td>голубыи</td>\n",
       "      <td>210175</td>\n",
       "      <td>2287623</td>\n",
       "      <td>A-</td>\n",
       "      <td>2287621</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>даите</td>\n",
       "      <td>даяти</td>\n",
       "      <td>210175</td>\n",
       "      <td>2287624</td>\n",
       "      <td>V-</td>\n",
       "      <td>2287618</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>съ</td>\n",
       "      <td>съ</td>\n",
       "      <td>210175</td>\n",
       "      <td>2287625</td>\n",
       "      <td>R-</td>\n",
       "      <td>2287624</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>людми</td>\n",
       "      <td>людие</td>\n",
       "      <td>210175</td>\n",
       "      <td>2287626</td>\n",
       "      <td>Nb</td>\n",
       "      <td>2287625</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>да</td>\n",
       "      <td>да</td>\n",
       "      <td>210175</td>\n",
       "      <td>2287627</td>\n",
       "      <td>G-</td>\n",
       "      <td>2287624</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>те</td>\n",
       "      <td>ти</td>\n",
       "      <td>210175</td>\n",
       "      <td>2287827</td>\n",
       "      <td>Df</td>\n",
       "      <td>2287630</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>съхҍ</td>\n",
       "      <td>соха</td>\n",
       "      <td>210175</td>\n",
       "      <td>2287628</td>\n",
       "      <td>Nb</td>\n",
       "      <td>2287630</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>не</td>\n",
       "      <td>не</td>\n",
       "      <td>210175</td>\n",
       "      <td>2287629</td>\n",
       "      <td>Df</td>\n",
       "      <td>2287630</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>кладе</td>\n",
       "      <td>класти</td>\n",
       "      <td>210175</td>\n",
       "      <td>2287630</td>\n",
       "      <td>V-</td>\n",
       "      <td>2287627</td>\n",
       "      <td>True</td>\n",
       "      <td>не</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>210175</td>\n",
       "      <td>2287829</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2287618</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Form      Lemma  Sentence ID  Token ID  POS  Head ID  Negation  \\\n",
       "597        а          а       210175   2287614   C-  2287829     False   \n",
       "598     четъ       чьто       210175   2287615   G-  2287829     False   \n",
       "599    ѡмьшҍ      омешь       210175   2287616   Nb  2287617     False   \n",
       "600   пришлю  присълати       210175   2287617   V-  2287615     False   \n",
       "601        и          и       210175   2287618   C-     <NA>     False   \n",
       "602       вꙑ         вы       210175   2287619   Pp  2287624     False   \n",
       "603      имъ          и       210175   2287620   Pp  2287624     False   \n",
       "604     къне       конь       210175   2287621   Nb  2287624     False   \n",
       "605      мъи        мои       210175   2287622   Pp  2287621     False   \n",
       "606  голубꙑи    голубыи       210175   2287623   A-  2287621     False   \n",
       "607    даите      даяти       210175   2287624   V-  2287618     False   \n",
       "608       съ         съ       210175   2287625   R-  2287624     False   \n",
       "609    людми      людие       210175   2287626   Nb  2287625     False   \n",
       "610       да         да       210175   2287627   G-  2287624     False   \n",
       "611       те         ти       210175   2287827   Df  2287630     False   \n",
       "612     съхҍ       соха       210175   2287628   Nb  2287630     False   \n",
       "613       не         не       210175   2287629   Df  2287630     False   \n",
       "614    кладе     класти       210175   2287630   V-  2287627      True   \n",
       "615      NaN        NaN       210175   2287829  NaN  2287618     False   \n",
       "\n",
       "    Negation_Marker  \n",
       "597                  \n",
       "598                  \n",
       "599                  \n",
       "600                  \n",
       "601                  \n",
       "602                  \n",
       "603                  \n",
       "604                  \n",
       "605                  \n",
       "606                  \n",
       "607                  \n",
       "608                  \n",
       "609                  \n",
       "610                  \n",
       "611                  \n",
       "612                  \n",
       "613                  \n",
       "614              не  \n",
       "615                  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_sentence_df[[\"Form\", \"Lemma\",\"Sentence ID\", \"Token ID\",\"POS\", \"Head ID\", \"Negation\", \"Negation_Marker\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df3a8405-2051-4b95-aaaa-69c0d5258161",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"OUTPUTS/dataframe_02_4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c86de5-ff87-4fbb-b4b8-80d312c850d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml-min)",
   "language": "python",
   "name": "ml-min"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
